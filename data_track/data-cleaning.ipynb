{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Real world data is usually messy and can contain errors. These can be in the form of unnecessary (sometimes empty) columns, values not formatted correctly or recorded erroneously.\n",
    "\n",
    "In this notebook we present a few data related issues and techniques on how to tackle these issues. We will first use the `student_debt.csv` data as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/student_debt.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the column `Unnamed: 0` does not contain any useful information, it is a mismatched version of the index only. This happens often when the data is downloaded, created or otherwise transformed as a side effect. \n",
    "\n",
    "We can easily get rid of a column in Pandas with the `drop()` method. The drop method accepts a list of column names in the columns attribute. \n",
    "Furthermore, you can pass `inplace = True` to perform the operation on the dataframe. If you do not do this, the operation returns a new dataframe without the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped_column = df.drop(columns=[\"Unnamed: 0\"]) #returns a new dataframe \n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True) #drops the column in the original dataframe\n",
    "## Note that now df_dropped_column and df is exactly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have only the useful columns, we can look at the values in our data set. \n",
    "For this a convenient starting point is looking at whether all data types are as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `Period` has a data type object, whereas it should be an integer or float. \n",
    "If you look at some of the year values, they have an asterisk * next to them. This is because those figures are preliminary only, but we still need those years in raw number format, such that we can perform analysis on it.\n",
    "\n",
    "We can use the `.apply()` function. This applies a custom function on every row of the dataframe its called upon.\n",
    "For simpler functions we can write a lambda function. Don't let the fancy name fool you, they are actually pretty easy.\n",
    "\n",
    "In python we can call the `replace()` method on any string to replace a piece of string to something user specified. We can use this to get rid of the * after the years. Then we can convert the years to integers with `int()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specifiy a lambda function which applies a certain operation on every row of our dataframe\n",
    "df.apply(lambda row: int(row['Period'].replace('*', '')), axis=1) \n",
    "#axis = 1 needed to apply the operations on every row, axis = 0 would apply the operations on every column, resulting in an error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it returns a new Pandas series, and does not modify the original dataframe. We can create or replace a column with pandas by simply specifying the column name and the values of the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Period\"] = df.apply(lambda row:int(row['Period'].replace('*', '')), axis=1) \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the data type of Period now changed to int64, which was our desired goal.\n",
    "\n",
    "# Missing values and outliers on Salary data set\n",
    "\n",
    "In some data sets you have missing or otherwise erroneous values. In general, missing values are very common in real life, and dealing with them is an important step in data cleaning.\n",
    "\n",
    "For this we will use a different data set `DenBosch-avg-salary.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salaries = pd.read_csv(\"data/DenBosch-avg-salary.csv\")\n",
    "df_salaries.iloc[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salaries.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, part of this data has missing values (in the form of `.`) in several places. Furthermore also notice that all the numerical values are listed as objects, because Python uses a `.` to indicate decimals, and in this data set we have `,` separating them. Hence, Pandas cannot convert e.g. the number 40,2 to a number, only if we modify it to 40.2\n",
    "\n",
    "First we have to do two things: replace the `,` to `.` in order to convert the numbers to `float`, and set the missing values to be actually missing, this latter can be done by filtering cells containing a `.` and setting their value to `np.nan`.\n",
    "\n",
    "For this exercise, we select only the columns we want to clean. In our case, we only want columns with numbers, so we can drop *AreaCode, Neighbourhood code* columns for imputation.\n",
    "\n",
    "Later on, we need to recombine (concatenate) the imputed columns with the other two we decided to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "columns_not_impute = ['AreaCode', 'Neighbourhood code'] # columns we don't want to preprocess further, as they are not numerical\n",
    "df_salaries_to_impute = df_salaries.drop(columns=columns_not_impute)\n",
    "\n",
    "#iterate over all columns and replace , with . using a lambda function\n",
    "for column in df_salaries_to_impute.columns:\n",
    "    df_salaries_to_impute[column] = df_salaries_to_impute.apply(lambda row:row[column].replace(',', '.'), axis=1)\n",
    "\n",
    "df_salaries_to_impute[df_salaries_to_impute.values == '.'] = np.nan #set the missing values to NaN\n",
    "\n",
    "df_salaries_to_impute = df_salaries_to_impute.astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the missing values we can do something called *missing value imputation*, which replaces missing values with some plausible value.\n",
    "For this we will use the Python library scikit-learn, that has multiple methods to impute missing values.\n",
    "In this tutorial we use the `SimpleImputer`, which imputes the average value of each column with missingness.\n",
    "NOTE: This imputation algorithm only accepts a dataframe that contains one specific data type. If we specify `SimpleImputer(strategy = 'mean')` we can impute numerical values only. However, if we specify `SimpleImputer(strategy = 'median')` we can fill in the missing values for textual fields.\n",
    "\n",
    "\n",
    "Before that we can look at how many missing values we have for each column, this can be done by using `isna()` that returns true if a cell is missing and False otherwise, and then `.sum()` over these values to get the number of missing values per column. In the code cell below we also divided by the number of rows, such that the values can be interpreted as percentages, but this is an optional step in this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_salaries_to_impute.isna().sum()/df_salaries_to_impute.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean') #initialize scikit learn imputer\n",
    "imputed_values = imputer.fit_transform(df_salaries_to_impute)\n",
    "### You can see that the imputation algorithm returns values only.\n",
    "### We can convert this into a DataFrame by doing:\n",
    "df_imputed_salaries = pd.DataFrame(imputed_values, columns=df_salaries_to_impute.columns, index=df_salaries_to_impute.index)\n",
    "# Here we specify that we want a new dataframe wiht the imputed values and the same columns and index as the previous data frame\n",
    "\n",
    "df_imputed_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is use `pd.concat()` to join the data with the discarded area codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salaries_cleaned = pd.concat([df_salaries[columns_not_impute], df_imputed_salaries], axis=1)\n",
    "df_salaries_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the cleaned dataframe to a csv file, so that we can access it later. \n",
    "For this we can call the `to_csv()` method of a dataframe.\n",
    "It is important that you specify `index = False` in the arguments, as we don't want to save indices, those are auto-generated by Pandas every time we open a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salaries_cleaned.to_csv(\"data/cleaned-DenBosch-avg-salary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "In these exercises you will perform similar cleaning tasks on several other data sets downloaded from our data dashboard.\n",
    "\n",
    "The first data set we will use is the `municipality_data.csv`.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Load the data and check which parts need cleaning. Look for extra columns, mismatching data types, check the percentage of missing values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ Exercise code comes here ------------- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Using the methods presented above, try and fix those inconsistencies. If you need to do missing value imputation, use the `SimpleImputer` with the correctly specified strategy.\n",
    "Remember the steps how to fill in missing values:\n",
    "- Check by hand which columns _should_ have numerical values.\n",
    "- Check data types with `.dtypes` whether in fact they are numerical.\n",
    "    - If not identify the cause (letters next to the numbers, dots for missing values instead of NaN etc.)\n",
    "    - Replace/remove text from all columns that should be numerical\n",
    "- Separate only numerical columns into a new dataframe\n",
    "- Perform the imputation using the `SimpleImputer` \n",
    "- Merge (concatenate) the imputed numerical values with the (excluded) non-numerical values to get the original dataframe back\n",
    "\n",
    "If you are stuck anywhere along the process, feel free to reach out to one of the Data mentors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Exercise code comes here ------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Save the cleaned dataframe to a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Exercise code comes here ------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning second data set\n",
    "\n",
    "Below we have some other practice material on another data set named `Farmers_NH3_emission.csv`.\n",
    "\n",
    "### Exercise 4\n",
    "Check the data types of the column. You should see that some data types do not match because of the units in the cells next to the numbers. Try to get rid of them and convert the columns to numbers (float or int as necessary).\n",
    "\n",
    "If you don't know how to start, reach out to a Data mentor, we are happy to help!\n",
    "\n",
    "As practice you can also check if the data contains any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Exercise code comes here ------------- #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('health')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1973c0181cad203b4324511b50a7a7200e1137d5fa7327ae47e76e06849030ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
